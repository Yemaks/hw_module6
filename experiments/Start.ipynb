{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../my_module/Start.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../my_module/Start.py\n",
    "\n",
    "\"\"\"\n",
    "Contains functionality for selecting most relevant vines and for summarization users description.\n",
    "\"\"\"\n",
    "\n",
    "from normalize_text import normalize\n",
    "from text_cleanup import cleanup\n",
    "from generate_summary import summary\n",
    "from search_by_description import top_search_by_description\n",
    "\n",
    "print(\"1: Obtaining a summarization from the description of wine\\n2: Obtaining the top-5 wines that best match the description\")\n",
    "choice = input(\"Select work option: \")\n",
    "while(True):\n",
    "    if choice == '1':\n",
    "        text = input(\"Enter text to summarize: \")\n",
    "        print(\"\\nOriginal Text:\\n\")\n",
    "        print(text)\n",
    "        print('\\nSummarized text:\\n')\n",
    "        print(summary(cleanup(normalize(text))[0], normalize(text)))\n",
    "        break\n",
    "    elif choice == '2':\n",
    "        text = input(\"Enter text to summarize: \")\n",
    "        top_search_by_description(summary(cleanup(normalize(text))[0], normalize(text)))\n",
    "        break\n",
    "    else:\n",
    "        choice = input(\"Invalid input format. Select one of the options below: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../my_module/normalize_text.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../my_module/normalize_text.py\n",
    "\n",
    "import re\n",
    "\n",
    "def normalize(text):\n",
    "    \"\"\"Normalizes text.\n",
    "\n",
    "    Remove all .*?<[^>]+>© and newline symbol from text.\n",
    "\n",
    "    Args:\n",
    "        text: text in str format.\n",
    "\n",
    "    Returns:\n",
    "        text in str format without .*?<[^>]+>© and newline symbol.\n",
    "    \n",
    "    Example usage:\n",
    "        test_text = normalize(text='Hello world!?')\n",
    "    \"\"\"\n",
    "    tm1 = re.sub('<pre>.*?</pre>', '', text, flags=re.DOTALL)\n",
    "    tm2 = re.sub('<[^>]+>©', '', tm1, flags=re.DOTALL)\n",
    "    return tm2.replace(\"\\n\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../my_module/text_cleanup.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../my_module/text_cleanup.py\n",
    "\n",
    "import pandas as pd\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words('english')\n",
    "\n",
    "punctuations = '!\"#$%&\\'()*+,-/:;<=>?@[\\\\]^_`{|}~©'\n",
    "# Define function to cleanup text by removing personal pronouns, stopwords, and puncuation\n",
    "def cleanup(docs):\n",
    "    \"\"\"Text cleanup.\n",
    "\n",
    "    Remove all punctuations and stopwords from text, also lemmatizes text.\n",
    "\n",
    "    Args:\n",
    "        text: text in str format.\n",
    "\n",
    "    Returns:\n",
    "        text in str format without all punctuations and stopwords, lemmatized.\n",
    "    \n",
    "    Example usage:\n",
    "        test_text = cleanup(text='Hello world!?')\n",
    "    \"\"\"\n",
    "    texts = []\n",
    "    doc = nlp(docs, disable=['parser', 'ner'])\n",
    "    tokens = [tok.lemma_.lower().strip() for tok in doc if tok.lemma_ != '-PRON-']\n",
    "    tokens = [tok for tok in tokens if tok not in stopwords and tok not in punctuations]\n",
    "    tokens = ' '.join(tokens)\n",
    "    texts.append(tokens)\n",
    "    return pd.Series(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../my_module/generate_summary.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../my_module/generate_summary.py\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def summary(cleaned_text):\n",
    "    \"\"\"Summarizes text.\n",
    "\n",
    "    Summarizes text according to TfidfVectorizer.\n",
    "\n",
    "    Args:\n",
    "        text: text in str format.\n",
    "\n",
    "    Returns:\n",
    "        text in str format sortedt and summarized according to TfidfVectorizer.\n",
    "    \n",
    "    Example usage:\n",
    "        test_text = summary(text='Hello world!?')\n",
    "    \"\"\"\n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "    sentences = sent_tokenize(cleaned_text)\n",
    "\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(sentences)\n",
    "    scores = tfidf_matrix.toarray().sum(axis=0)\n",
    "    ranked_sentences = sorted(((scores[i], sentence) for i, sentence in enumerate(sentences)), reverse=True)\n",
    "    summary_sentences = [sentence for score, sentence in ranked_sentences[:3]]\n",
    "\n",
    "    original_sentences = [sentences[sentences.index(summary_sentence)] for summary_sentence in summary_sentences]\n",
    "\n",
    "    summary = ' '.join(original_sentences)\n",
    "\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../my_module/search_by_description.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../my_module/search_by_description.py\n",
    "\n",
    "import pandas as pd\n",
    "from gensim.models.doc2vec import Doc2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "model_downloaded = Doc2Vec.load(\"../models/d2v.model\")\n",
    "reviews_with_summary = pd.read_csv(\"../data/wine_reviews_with_summary.csv\", index_col=0, encoding='utf-8')\n",
    "reviews_with_summary['summary'] = reviews_with_summary['summary'].astype(str)\n",
    "\n",
    "def top_search_by_description(description, amount = 5):\n",
    "    \"\"\"Select most relevant vines from dataset according to users description.\n",
    "\n",
    "    Tokenized description and find most relevant vines from Dos2Vec dataset.\n",
    "\n",
    "    Args:\n",
    "        text: text in str format.\n",
    "        amount: number of vines listed.\n",
    "\n",
    "    Returns:\n",
    "        print amount of vines relevant to description.\n",
    "    \n",
    "    Example usage:\n",
    "        test_text = top_search_by_description(description='Hello world!?', amount = 5)\n",
    "    \"\"\"\n",
    "    #to find the vector of a document which is not in training data\n",
    "    description = word_tokenize(description)\n",
    "    description_vec = model_downloaded.infer_vector(description)\n",
    "    sims = model_downloaded.dv.most_similar([description_vec], topn=len(model_downloaded.dv))\n",
    "    print('The most suitable wines according to the description:\\n')\n",
    "    for i in range(5):\n",
    "        index = int(sims[i][0])\n",
    "        acc = float(sims[i][1]) * 100\n",
    "        print(f\"Vine title: {reviews_with_summary['title'][index]}, vine variety: {reviews_with_summary['variety'][index]}\"\n",
    "            f\", WineEnthusiast points: {reviews_with_summary['points'][index]:.0f}. Coincidence: {acc:.2f}%.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
